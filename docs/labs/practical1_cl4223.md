# LAB1

#### 1. 
The impact of Batch size is directly related to training speed of the model. Large batch size means it can use more data in each updates to accelerate the process of a training. In addition, the batch size also has influnence on accuracy. Too Large batch size and too small batch size will lead to poor training and validation accuracy. Because too small batch size will lead to slow converge which means it is hard to find a general optimal solution and too large batch size might find a local optimal solution instead of a general one.

#### 2. 
Varing max epochs will have influnence on the model performance (loss and accuracy) with the increasing epochs the model will be overfitting and with less epochs the model will be underfitting. It also makes impact on training time. More epochs more training time.

#### 3. 
Large learning rate will lead to fast converge and less traing time whereas small learning rate will lead to slow converge and more traing time. Because learning rate means very updates' step along the local optimal direction. Large learning rate will take large step towards the min loss direction. However, large learning rate will casue overshotting during training and small learning rate is stable converge but need more iterations or epochs to close the optimal solution. In this task when increasing the learning rate the accuracy is increasing at the initial period when it gets too large like 1, the accuracy became poor.

In term of the relation ship between Batch size and learning rate. For a small batch size, a relavent large learning rate eg: Batch size is 64 and learing rate is 0.01 in this task. The traing time is much quicker than the default learning rate and with higher training and validation accuracy. 




#### 4. 
Total param:  5.5 K
```python
class JSC_lch(nn.Module):
    def __init__(self, info):
        super(JSC_lch, self).__init__()

        # # Assuming input dimensions and number of classes from 'info'
        # num_classes = info.get('num_classes', 10)  # default to 10 classes if not specified
        # input_features = info.get('input_features', 16)  # default input feature size

        self.seq_blocks = nn.Sequential(
                        
            nn.BatchNorm1d(16),  # input_quant 
            nn.ReLU(16),
            nn.Linear(16, 32),           # Second layer
            nn.ReLU(32),
            nn.Linear(32, 64),          # Third layer
            nn.ReLU(64),
            nn.Linear(64, 32),          # Fourth layer
            nn.BatchNorm1d(32),
            nn.ReLU(32),
            nn.Linear(32, 16),           # Fifth layer
            nn.BatchNorm1d(16),
            nn.ReLU(16),
            nn.Linear(16, 5), 

            nn.ReLU(5),    # Output layer
        )

    def forward(self, x):
        return self.seq_blocks(x)
```

#### 5. 

```
JSC-Tiny

117       Trainable params
0         Non-trainable params
117       Total params
0.000     Total estimated model params size (MB)
Epoch 8: 100%|█| 3084/3084 [00:21<00:00, 142.29it/s, v_num=11, 
train_acc_step=0.561, val_acc_epoch=0.533, val_loss_epoch
----------------------------------------------------------------
JSC-lch
5.5 K     Trainable params
0         Non-trainable params
5.5 K     Total params
0.022     Total estimated model params size (MB)
Epoch 7: 100%|█| 3084/3084 [00:25<00:00, 122.02it/s, v_num=5, 
train_acc_step=0.719, val_acc_epoch=0.693, val_loss_epoch=
```
From the result the model created by my own has more better performance than the tiny one. The lch model's validation accuracy is 69.3%


# LAB2

 #### 1. report_graph_analysis_pass: 
Generates a report for the graph analysis and prints out an overall of the model in a table include the sequencial blocks and its function and operation such as ReLu, Linear operation and its input and output features. `placeholder`: starting for inputs. `gett_attr`: used for retrieving a parameter. `call_function`: used for display the number of function it used such as sum. In this representation, each seq_block such as ReLu and Linear are recorded by `call_module`. `output` is the final output in the model.

```
graph():
    %x : [num_users=1] = placeholder[target=x]
    %seq_blocks_0 : [num_users=1] = call_module[target=seq_blocks.0](args = (%x,), kwargs = {})
    %seq_blocks_1 : [num_users=1] = call_module[target=seq_blocks.1](args = (%seq_blocks_0,), kwargs = {})
    %seq_blocks_2 : [num_users=1] = call_module[target=seq_blocks.2](args = (%seq_blocks_1,), kwargs = {})
    %seq_blocks_3 : [num_users=1] = call_module[target=seq_blocks.3](args = (%seq_blocks_2,), kwargs = {})
    %seq_blocks_4 : [num_users=1] = call_module[target=seq_blocks.4](args = (%seq_blocks_3,), kwargs = {})
    %seq_blocks_5 : [num_users=1] = call_module[target=seq_blocks.5](args = (%seq_blocks_4,), kwargs = {})
    %seq_blocks_6 : [num_users=1] = call_module[target=seq_blocks.6](args = (%seq_blocks_5,), kwargs = {})
    %seq_blocks_7 : [num_users=1] = call_module[target=seq_blocks.7](args = (%seq_blocks_6,), kwargs = {})
    %seq_blocks_8 : [num_users=1] = call_module[target=seq_blocks.8](args = (%seq_blocks_7,), kwargs = {})
    %seq_blocks_9 : [num_users=1] = call_module[target=seq_blocks.9](args = (%seq_blocks_8,), kwargs = {})
    %seq_blocks_10 : [num_users=1] = call_module[target=seq_blocks.10](args = (%seq_blocks_9,), kwargs = {})
    %seq_blocks_11 : [num_users=1] = call_module[target=seq_blocks.11](args = (%seq_blocks_10,), kwargs = {})
    %seq_blocks_12 : [num_users=1] = call_module[target=seq_blocks.12](args = (%seq_blocks_11,), kwargs = {})
    %seq_blocks_13 : [num_users=1] = call_module[target=seq_blocks.13](args = (%seq_blocks_12,), kwargs = {})
    return seq_blocks_13Network overview:
{'placeholder': 1, 'get_attr': 0, 'call_function': 0, 'call_method': 0, 'call_module': 14, 'output': 1}
Layer types:
[BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Linear(in_features=16, out_features=32, bias=True), ReLU(inplace=True), Linear(in_features=32, out_features=64, bias=True), ReLU(inplace=True), Linear(in_features=64, out_features=32, bias=True), BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Linear(in_features=32, out_features=16, bias=True), BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Linear(in_features=16, out_features=5, bias=True), ReLU(inplace=True)]
```

#### 2.1. `profile_statistics_analysis_pass()`: 
Perform profile statistics analysis on the given graph (mg). It analyzes the model graph (mg) to gather statistics about the model's layers and operations. These statistics include information such as the number of parameters in each layer, memory usage, computational cost, and execution time. The pass_args argument could be for providing additional options or configurations for the profiling.
After execution, this function updates the model graph with profiling information which can be used for optimization, debugging, or obsever the model's performance characteristics.

#### 2.2. `report_node_meta_param_analysis_pass()`:
Perform meta parameter analysis on the nodes in the graph and generate a report. This function is used to generate a report based on the analysis of meta-parameters of nodes in the model graph. The mg parameter is the model graph, similar to the previous function. The {"which": ("software",)} argument of the function is filtering and focusing the analysis on certain types of nodes or aspects of the model, those that are software-related. like data precision, weight. The purpose of this function is to provide insights into specific parts of the model, such as how certain parameters are set or how different layers are configured.

#### 3. 
Because for jsc-tiny model there is only one linear module and the pass_args only change the linear module therefore only the linear  1 OP changed.

#### 4.

```python
pass_args = {
"by": "type",
"default": {"config": {"name": None}},
"linear": {
        "config": {
            "name": "integer",
            # data
            "data_in_width": 8,
            "data_in_frac_width": 4,
            # weight
            "weight_width": 8,
            "weight_frac_width": 4,
            # bias
            "bias_width": 8,
            "bias_frac_width": 4,
        }
},
}

from chop.passes.graph.transforms import (
    quantize_transform_pass,
    summarize_quantization_analysis_pass,
)
from chop.ir.graph.mase_graph import MaseGraph


ori_mg = MaseGraph(model=model)
ori_mg, _ = init_metadata_analysis_pass(ori_mg, None)
ori_mg, _ = add_common_metadata_analysis_pass(ori_mg, {"dummy_in": dummy_in})

mg, _ = quantize_transform_pass(mg, pass_args)
summarize_quantization_analysis_pass(ori_mg, mg, save_dir="quantize_summary")

```


#### 5.
For my own model, the 5 linear modules are cahnged.
```
[Quantized graph histogram:[0m
[INFO    [0m [34m
| Original type   | OP           |   Total |   Changed |   Unchanged |
|-----------------+--------------+---------+-----------+-------------|
| BatchNorm1d     | batch_norm1d |       3 |         0 |           3 |
| Linear          | linear       |       5 |         5 |           0 |
| ReLU            | relu         |       6 |         0 |           6 |
| output          | output       |       1 |         0 |           1 |
| x               | placeholder  |       1 |         0 |           1 |

```

#### 6.

The following is the weight proved code to calculate each node's weight precision. From its output, the weight indeed be quatized.
```python
from chop.passes.graph.analysis.quantization.calculate_avg_bits import calculate_avg_bits_mg_analysis_pass
from chop.ir.graph.mase_graph import MaseGraph


def calculate_bits_mg_analysis_pass(graph, pass_args: dict):

    for node in graph.fx_graph.nodes:
        mase_meta = node.meta["mase"].parameters
        mase_op = mase_meta["common"]["mase_op"]
        mase_type = mase_meta["common"]["mase_type"]

    
        if mase_type in ["module", "module_related_func"]:
           if mase_op in ["linear", "conv2d", "conv1d"]:
              w_meta = mase_meta["common"]["args"]["weight"]
              # Display the weight metadata
              print(f"Operation: {mase_op}, Node: {node.name}")
              print(f"  Weight Shape: {w_meta['shape']}")
              print(f"  Precision: {w_meta['precision']} bits")
              print(f"  Weight : {w_meta['value']}")
              print()

print ("original graph")
calculate_bits_mg_analysis_pass(ori_mg, {})

print("modified grapgh")
calculate_bits_mg_analysis_pass(mg, pass_args)
```
Out put: From the output, the weights indeed be quatized.
```
original graph
Operation: linear, Node: seq_blocks_2
  Weight Shape: [32, 16]
  Precision: [32] bits
  Weight : Parameter containing:
tensor([[-9.0030e-03,  1.3705e-01, -2.0550e-01, -1.8825e-01, -9.9253e-02,
          6.3005e-02, -7.9092e-03,  1.9481e-01, -2.5130e-02,  6.6416e-02,
         -7.3933e-02, -4.9696e-02, -2.3817e-01, -1.6699e-01, -1.0155e-01,
          1.4870e-02],
        [ 8.1966e-02,  1.5976e-01, -1.6174e-01, -1.0140e-01,  9.5657e-02,
          2.0798e-01, -4.9287e-02,  1.8534e-01, -3.8132e-02,  3.3299e-02,
          2.3485e-01, -2.2178e-01, -1.4854e-01, -5.7424e-02, -8.0627e-02,
          2.3284e-01],
        [-1.6088e-01, -1.2642e-01, -1.7955e-01, -2.3827e-01, -1.5002e-01,
          2.1309e-01,  1.0271e-01,  1.1824e-01,  4.3221e-03, -1.3177e-01,
          3.7039e-02, -2.3456e-01, -1.8739e-01, -1.2989e-01,  1.5085e-01,
          1.3757e-01],
        [-1.0339e-01, -2.0419e-02,  1.5736e-01,  2.4880e-01,  9.3352e-02,
          3.2795e-02,  1.5991e-01, -1.4515e-01,  3.8902e-02, -1.9798e-01,
         -1.8125e-01, -1.3133e-01,  1.0701e-01,  1.0171e-01, -1.5516e-01,
          6.5267e-02],
        [ 1.5294e-01, -2.9883e-02,  1.7127e-02,  6.7227e-02,  1.5244e-01,
          2.3664e-01, -1.9560e-01, -8.8826e-02,  9.5233e-02,  2.1759e-01,
          2.2949e-01,  2.3255e-01,  5.9521e-02, -2.0677e-01,  2.8521e-02,
         -1.5645e-01],
        [-2.1759e-01,  2.1914e-01,  1.8201e-01, -2.5681e-01,  5.1117e-02,
         -4.2812e-02, -3.9276e-02, -1.1080e-01,  9.7993e-02, -1.4894e-01,
          8.8584e-02,  1.2425e-01,  1.7668e-01,  9.3587e-02, -2.5786e-01,
         -1.7253e-01],
        [ 1.1889e-01,  4.3405e-02, -1.9846e-01, -1.4064e-01,  2.3663e-01,
          1.7118e-01, -9.9029e-02, -5.7003e-02, -2.2814e-01, -5.9339e-03,
         -1.9022e-01, -2.0209e-01, -1.5154e-02,  3.2674e-02, -1.0754e-01,
          1.5561e-01],
        [-1.4885e-01,  2.2133e-01,  1.7193e-01, -2.2944e-01, -5.2584e-02,
          1.0350e-02,  3.4796e-02,  5.6848e-02,  9.6400e-02,  1.0708e-02,
         -1.2644e-01,  1.1302e-01, -2.4234e-01, -1.4818e-01, -4.7940e-02,
         -1.2908e-01],
        [-9.3874e-02, -1.9739e-01, -4.7943e-02,  5.7088e-02, -1.6614e-01,
         -1.5875e-02,  1.7030e-01, -3.6420e-02, -1.7051e-03, -2.5839e-02,
          4.9228e-02,  1.4966e-01,  2.3250e-01,  1.4789e-01,  2.4547e-01,
         -7.8189e-04],
        [-2.2420e-01, -1.1860e-01,  1.6727e-01, -3.7974e-03, -1.2682e-01,
         -1.9142e-01, -2.3513e-01, -2.1135e-01, -5.1853e-02,  1.3744e-01,
          1.3559e-01, -2.4178e-01,  1.5616e-01, -1.9739e-01, -5.5996e-02,
         -1.0148e-01],
        [-3.3341e-02, -3.2794e-02, -2.2207e-01, -2.1606e-01, -4.4976e-02,
          4.3206e-03, -1.1306e-01,  8.9712e-02, -2.2456e-01, -1.5379e-02,
          2.2559e-01, -9.7292e-02,  2.2623e-01,  8.7381e-02, -2.3474e-01,
          1.5997e-01],
        [-3.3836e-02, -1.0127e-01,  2.0991e-01, -1.9179e-01,  3.3263e-02,
         -4.5728e-02,  1.7388e-01,  7.2055e-02,  1.1547e-01,  9.7147e-02,
         -5.0056e-02, -4.1491e-02, -1.9866e-01,  1.4438e-01,  2.0847e-01,
          1.8193e-01],
        [-1.7403e-01,  2.5624e-02, -1.7155e-01, -1.3383e-01, -1.3925e-01,
          8.9459e-02, -1.4006e-01,  2.3239e-04,  1.9385e-02,  1.7184e-01,
         -1.7662e-01, -1.6408e-01, -1.3386e-01,  1.8088e-01, -8.1287e-02,
          2.2398e-01],
        [ 9.8997e-02,  1.8008e-02, -1.0817e-02, -5.6263e-02,  2.0138e-02,
         -6.5735e-02,  8.1927e-03,  4.1148e-02, -1.8563e-01, -1.2886e-01,
          2.0191e-01, -2.0133e-01, -1.6258e-02,  2.5038e-01,  8.4480e-02,
         -6.0294e-03],
        [-2.2787e-01,  1.3608e-01, -1.6623e-01, -6.8743e-02, -8.5199e-02,
         -3.8130e-02, -5.1891e-03,  2.0051e-01,  2.3328e-02,  2.1116e-01,
          1.3864e-01, -1.6856e-01,  1.0095e-01, -1.8278e-01, -8.8565e-02,
          9.3488e-02],
        [ 8.7201e-02,  2.0342e-01, -7.8192e-02, -9.7219e-03,  1.1948e-01,
         -2.4267e-01,  1.7632e-01, -2.1071e-01, -9.5776e-04, -4.8345e-02,
         -1.3231e-01,  3.2720e-02,  2.0143e-01, -7.7577e-02, -1.6302e-01,
         -9.2182e-02],
        [-2.4017e-01,  1.0430e-01, -1.3650e-01, -1.8366e-01, -1.6096e-01,
          1.2586e-01,  1.3278e-01,  1.8892e-01,  9.1065e-02, -7.9640e-02,
         -6.3947e-02,  7.9761e-02,  2.0957e-01,  6.9434e-02, -1.2723e-01,
         -1.1047e-01],
        [-2.4454e-01,  5.2603e-02, -1.3482e-01, -2.1806e-01,  2.2254e-01,
         -1.5793e-01, -3.3097e-02,  6.7401e-02,  3.3155e-03, -1.7307e-01,
         -2.0691e-01,  1.9272e-01,  3.5453e-02,  2.0103e-01, -8.1247e-02,
          7.3574e-02],
        [-5.9546e-02, -2.9777e-04, -1.3844e-01,  9.5576e-02,  8.6778e-02,
         -6.0789e-04, -6.2878e-02, -1.6070e-01,  1.6623e-01, -1.8303e-01,
          1.0441e-01, -9.4931e-02, -1.2180e-01,  3.5146e-02, -1.1472e-01,
          7.2623e-02],
        [ 5.2124e-02, -1.7345e-01,  5.4545e-02,  1.0058e-01,  1.0526e-01,
         -3.6936e-02, -2.0369e-01, -4.1967e-02,  8.8015e-02, -9.3865e-02,
          8.9661e-02,  1.6263e-01, -1.3156e-01,  2.7352e-03,  1.2162e-01,
          8.6600e-03],
        [ 9.4698e-03,  3.5839e-02, -1.9789e-01,  1.8059e-02,  1.6945e-01,
          2.1948e-01,  1.3631e-01,  2.6281e-02,  1.0615e-01, -1.2196e-01,
         -2.0328e-01, -2.1145e-01,  2.4492e-01,  1.5578e-01, -1.7150e-01,
          1.0570e-01],
        [ 1.8421e-01,  2.5270e-01,  2.3101e-01,  2.0879e-01, -4.6305e-02,
         -7.4271e-02,  2.1266e-01,  1.4777e-01, -1.4307e-01,  2.2891e-01,
          1.2329e-01,  1.4279e-01, -1.5354e-01,  7.6429e-02, -7.8919e-02,
          1.9116e-01],
        [-4.2149e-02,  8.8812e-02,  3.4064e-02,  9.6485e-02, -9.7580e-02,
          1.0896e-01, -1.8864e-01,  4.3151e-02, -1.8313e-01, -2.5387e-01,
         -2.0415e-01,  1.9247e-01,  1.2788e-01,  2.2576e-01,  1.9010e-01,
         -2.2615e-01],
        [-1.7775e-01, -4.2131e-02, -1.5358e-01,  1.8578e-01, -1.7537e-01,
         -1.0569e-01, -2.3700e-01, -1.4215e-01,  2.1015e-01,  1.9921e-01,
          1.7211e-01,  1.8913e-01,  2.1946e-01, -6.6247e-02,  1.2121e-01,
          2.3719e-01],
        [ 8.4229e-02,  2.4664e-01,  1.4678e-01,  1.7022e-01, -7.0026e-02,
          1.3590e-01,  2.8578e-02, -5.5983e-02, -1.4115e-01, -1.3557e-01,
         -1.8956e-01,  1.6986e-01,  1.7844e-01, -2.4849e-02, -1.2691e-01,
          1.9674e-01],
        [ 1.7223e-01,  2.6345e-02, -1.1778e-01,  2.3956e-01,  1.0558e-01,
         -1.8032e-01,  2.4649e-01,  1.9622e-01, -8.3943e-02,  1.5193e-01,
         -1.2898e-01, -2.4810e-02,  2.2458e-01,  2.1182e-02, -1.9032e-01,
         -7.9325e-02],
        [-6.2945e-02, -5.8449e-02,  1.2532e-02,  2.2075e-01,  5.1885e-03,
         -1.5911e-01,  1.7987e-02,  1.2483e-01,  1.2917e-01, -2.2481e-01,
         -4.7599e-02, -1.8618e-01, -9.8850e-02,  9.3453e-02, -1.8749e-01,
          8.6012e-02],
        [ 2.2052e-01,  3.0461e-02, -1.8479e-01, -9.5754e-02,  3.4946e-02,
         -1.9337e-01,  1.2684e-01, -2.1125e-01, -2.3775e-01, -2.2563e-01,
         -3.1572e-02,  1.8658e-01, -2.2164e-01,  2.2158e-01, -1.2895e-01,
          6.1084e-02],
        [ 1.4914e-02, -2.2653e-01,  2.0328e-01,  1.2580e-01,  2.4016e-01,
          1.1736e-01, -1.7123e-01,  2.0400e-01,  7.2306e-03,  1.1133e-01,
         -2.0845e-01, -7.7777e-02, -2.5254e-01, -1.0069e-01,  4.6139e-02,
         -2.0884e-01],
        [ 6.6918e-02,  1.2076e-01,  1.0986e-02, -1.6996e-01, -2.0153e-01,
         -7.4052e-02,  1.1630e-01,  2.5566e-01,  1.5033e-01, -1.6999e-02,
          9.7010e-02, -2.3539e-01, -2.0306e-01,  1.2628e-01, -1.6866e-01,
          1.0296e-01],
        [-1.8699e-01, -1.1455e-01, -2.0500e-01,  1.2826e-01, -2.9339e-02,
          1.8750e-01,  1.4732e-01,  1.3571e-01,  6.4906e-04, -7.0374e-02,
         -4.2374e-02,  4.5121e-02,  1.3007e-01, -1.6805e-01,  2.0352e-01,
         -1.4589e-02],
        [-1.9949e-01, -1.5180e-01,  2.0691e-01,  2.1497e-01, -2.4268e-01,
          1.6746e-01,  1.2733e-01,  1.4887e-01,  2.1369e-01, -1.4650e-01,
          6.4244e-02,  8.9601e-02, -8.5666e-02,  7.4637e-02, -8.5341e-02,
          1.3757e-01]], requires_grad=True)

Operation: linear, Node: seq_blocks_4
  Weight Shape: [64, 32]
  Precision: [32] bits
  Weight : Parameter containing:
tensor([[ 0.0224,  0.1474, -0.1240,  ..., -0.1188, -0.0096,  0.1450],
        [-0.0568, -0.1641,  0.0617,  ..., -0.0809,  0.0947, -0.0754],
        [-0.0868, -0.0162, -0.0158,  ..., -0.1618, -0.1322,  0.0759],
        ...,
        [ 0.1006,  0.1655, -0.1205,  ..., -0.1700,  0.1280, -0.1308],
        [-0.0044,  0.0273,  0.0577,  ..., -0.0340,  0.0031, -0.0549],
        [-0.1120, -0.1265,  0.1155,  ...,  0.1334,  0.0941,  0.0208]],
       requires_grad=True)

Operation: linear, Node: seq_blocks_6
  Weight Shape: [32, 64]
  Precision: [32] bits
  Weight : Parameter containing:
tensor([[-0.0243, -0.0506,  0.0695,  ..., -0.1145, -0.0460, -0.0724],
        [-0.0733,  0.1067, -0.0335,  ...,  0.1061, -0.0300,  0.0499],
        [-0.0338, -0.0401, -0.0624,  ..., -0.0611, -0.1121,  0.0790],
        ...,
        [-0.0274, -0.0959,  0.1080,  ..., -0.0567,  0.0701,  0.0236],
        [-0.0093, -0.1025,  0.1283,  ...,  0.0770, -0.0065,  0.0090],
        [ 0.1003,  0.0592,  0.0989,  ..., -0.1072,  0.0992,  0.0348]],
       requires_grad=True)

Operation: linear, Node: seq_blocks_9
  Weight Shape: [16, 32]
  Precision: [32] bits
  Weight : Parameter containing:
tensor([[-0.0151,  0.0629, -0.0714, -0.0691, -0.0203, -0.0949,  0.1023,  0.0968,
         -0.1384,  0.0510, -0.0721, -0.0949, -0.0323, -0.0455, -0.1065,  0.0245,
          0.0987, -0.0396, -0.0278, -0.0007, -0.0840,  0.0887,  0.1648, -0.0130,
          0.0069,  0.1275,  0.1390, -0.1419, -0.0927, -0.1654,  0.1248,  0.1437],
        [-0.0658,  0.0789,  0.0326, -0.1397,  0.0642,  0.0934,  0.0238,  0.0045,
          0.0313,  0.1326,  0.1501,  0.0227,  0.0810, -0.0324,  0.0855, -0.0628,
         -0.1457,  0.1041,  0.0805,  0.0192,  0.0710,  0.0154,  0.0509,  0.0735,
         -0.0893, -0.0252,  0.0180, -0.0065, -0.1466,  0.0299,  0.0932, -0.0981],
        [ 0.1565,  0.0520,  0.0978, -0.0765, -0.1412, -0.0772, -0.0025, -0.0899,
          0.1480, -0.0896, -0.1143, -0.0425,  0.1770, -0.0742,  0.1528,  0.0750,
         -0.0059,  0.0884,  0.0907, -0.1024, -0.0209,  0.1724,  0.1228, -0.0923,
         -0.0324,  0.0889,  0.0760,  0.0059, -0.1011, -0.1354, -0.0193,  0.0542],
        [-0.0200, -0.1310, -0.0266,  0.1597, -0.1529, -0.1263,  0.1196,  0.1858,
         -0.1798, -0.1422, -0.0731,  0.1544, -0.0881,  0.0095,  0.1291, -0.0803,
         -0.0366, -0.1018,  0.0222,  0.1851,  0.0545,  0.1140, -0.0045, -0.0469,
         -0.0414,  0.1163, -0.0095, -0.1522, -0.0523,  0.0696,  0.1794, -0.0509],
        [-0.1382, -0.1315, -0.1442,  0.0875,  0.1401,  0.0456,  0.0647,  0.1006,
          0.1238,  0.0991,  0.1107,  0.0414,  0.0973, -0.1151, -0.1111,  0.0600,
          0.0185, -0.1485, -0.1188, -0.1582,  0.1564,  0.1057, -0.0999,  0.0382,
         -0.0231,  0.1406, -0.1751, -0.1075, -0.1470,  0.0835, -0.0936,  0.1097],
        [ 0.0626, -0.1014, -0.0364, -0.0902,  0.0924, -0.0889,  0.0429, -0.0091,
         -0.1215, -0.0918, -0.1028,  0.1008,  0.0439,  0.0003, -0.0923, -0.1558,
         -0.0652, -0.1485, -0.1750, -0.1855, -0.0972, -0.1646,  0.0432,  0.0901,
          0.0762, -0.1974,  0.0887,  0.0167,  0.0751, -0.0515,  0.0972, -0.1536],
        [ 0.0655,  0.1255,  0.0661, -0.1723, -0.0475, -0.0333,  0.0819, -0.0542,
          0.0266, -0.0720, -0.0238, -0.0048, -0.1071, -0.1280,  0.1577,  0.1392,
         -0.0157,  0.1461,  0.1293, -0.0956, -0.0626, -0.1385,  0.0498,  0.0158,
         -0.0263,  0.1478, -0.0562, -0.0991, -0.1209, -0.0933,  0.1902, -0.0758],
        [-0.0368,  0.0696, -0.1362, -0.1170,  0.1465, -0.0721, -0.0935, -0.0378,
         -0.0129,  0.0358,  0.0771, -0.1423,  0.1643, -0.1603,  0.0631, -0.0421,
         -0.0890,  0.1581, -0.1576,  0.0051,  0.1282,  0.1414, -0.1480,  0.0417,
          0.1689, -0.1849,  0.0312, -0.0990,  0.0222, -0.1647,  0.1000, -0.0320],
        [ 0.1146, -0.0091, -0.0351, -0.0476, -0.1503, -0.0691,  0.0015, -0.1197,
          0.1609, -0.1300, -0.0591, -0.0349, -0.1566, -0.0044, -0.0723, -0.1351,
         -0.0097,  0.1425,  0.0303,  0.1626,  0.0047, -0.0638,  0.1236, -0.0868,
         -0.1757,  0.0484,  0.1049, -0.1106,  0.0500, -0.0494, -0.0078, -0.0305],
        [-0.0202, -0.1629,  0.0483, -0.0642,  0.1468,  0.0174, -0.1626, -0.1075,
         -0.0812,  0.0886, -0.1110, -0.0738,  0.0252, -0.1380, -0.0633,  0.1717,
         -0.0942, -0.0729,  0.1161,  0.0638,  0.1269, -0.1372,  0.0897, -0.0232,
         -0.1253, -0.0487, -0.1488,  0.1014, -0.0254, -0.1719,  0.1710,  0.1225],
        [ 0.0839,  0.1723,  0.1386,  0.1747,  0.1628,  0.1451,  0.0459, -0.0204,
          0.0132, -0.0960,  0.0984, -0.0021,  0.0271, -0.1074, -0.0678,  0.1560,
          0.0073,  0.0557, -0.1602, -0.0814,  0.0037, -0.0177,  0.0895, -0.0300,
          0.1443, -0.0425,  0.0662, -0.1224,  0.0189, -0.1665, -0.1568, -0.0945],
        [ 0.0568,  0.0228,  0.0098, -0.0138,  0.1574,  0.1922,  0.0849,  0.1300,
          0.0007, -0.0751,  0.0438,  0.1041, -0.0373, -0.1119,  0.0512, -0.0393,
         -0.0786,  0.1194, -0.0060, -0.1116, -0.0905,  0.1706,  0.1012, -0.1143,
          0.0028, -0.0156,  0.0844, -0.1295, -0.0092,  0.0565,  0.0756,  0.0222],
        [-0.0757, -0.1542,  0.1146,  0.1393, -0.0704, -0.1658,  0.0355, -0.1351,
         -0.1003,  0.0667, -0.0120, -0.0011,  0.1601, -0.0113,  0.1014, -0.0453,
          0.1321,  0.0360,  0.1462,  0.0508, -0.0727,  0.0812,  0.1630,  0.0530,
          0.0735, -0.1327,  0.0816, -0.1886,  0.1394,  0.0904,  0.0306, -0.1466],
        [ 0.1746,  0.1248,  0.1650,  0.0889,  0.0530, -0.0394, -0.0519,  0.0675,
          0.0529,  0.0109,  0.0115,  0.0594, -0.1551, -0.1586, -0.0688,  0.1362,
          0.0927, -0.1487, -0.0316, -0.0225,  0.0452,  0.1456,  0.0085, -0.1386,
          0.0140, -0.0702, -0.0972,  0.1189, -0.0094,  0.1007,  0.1170,  0.0286],
        [ 0.1043, -0.0646,  0.1036,  0.0130,  0.1028,  0.1769, -0.1129,  0.1571,
          0.1234, -0.1320,  0.0350, -0.1474,  0.0612,  0.0264, -0.1839,  0.0619,
         -0.0401,  0.1868,  0.1836,  0.0498,  0.1229,  0.0531,  0.1748, -0.0223,
          0.0508, -0.0382, -0.0036, -0.0327, -0.0596,  0.1098,  0.0628, -0.0763],
        [-0.1399,  0.1045, -0.1454, -0.0053, -0.1587,  0.1436,  0.1536, -0.1591,
         -0.1658, -0.0865,  0.0476, -0.0798, -0.0315, -0.0396,  0.0192, -0.0796,
          0.0950, -0.1186, -0.0061,  0.1325, -0.0947,  0.1295,  0.1021, -0.0390,
          0.0742, -0.1366,  0.0024,  0.0607,  0.1604, -0.0910, -0.1119, -0.0864]],
       requires_grad=True)

Operation: linear, Node: seq_blocks_12
  Weight Shape: [5, 16]
  Precision: [32] bits
  Weight : Parameter containing:
tensor([[-0.0514,  0.0585,  0.0570, -0.0256, -0.1943,  0.2167,  0.1327, -0.2331,
         -0.1954,  0.2544,  0.0489, -0.0591,  0.1941,  0.0809,  0.1007,  0.2129],
        [-0.0302,  0.2325, -0.1862, -0.1640,  0.0371, -0.0474,  0.1380, -0.0931,
          0.0352, -0.0756,  0.1955, -0.0105, -0.0100,  0.1886,  0.2487,  0.2606],
        [-0.2664, -0.1637, -0.0359, -0.0510,  0.1750,  0.1313, -0.0324,  0.0333,
          0.0387,  0.2344,  0.1532, -0.1167, -0.1291,  0.1081, -0.0146, -0.1572],
        [-0.2620,  0.0460, -0.0914, -0.1494,  0.0692,  0.2619,  0.2250,  0.1708,
         -0.2033,  0.1608,  0.1204,  0.0763, -0.0810, -0.0036, -0.1394, -0.0677],
        [-0.0067, -0.0162, -0.2327,  0.1228, -0.1875, -0.1668,  0.1342, -0.1909,
          0.2721,  0.2633, -0.0714, -0.0170, -0.1781,  0.0303,  0.0653, -0.1263]],
       requires_grad=True)

modified grapgh
Operation: linear, Node: seq_blocks_2
  Weight Shape: [32, 16]
  Precision: [8, 4] bits
  Weight : Parameter containing:
tensor([[-9.0030e-03,  1.3705e-01, -2.0550e-01, -1.8825e-01, -9.9253e-02,
          6.3005e-02, -7.9092e-03,  1.9481e-01, -2.5130e-02,  6.6416e-02,
         -7.3933e-02, -4.9696e-02, -2.3817e-01, -1.6699e-01, -1.0155e-01,
          1.4870e-02],
        [ 8.1966e-02,  1.5976e-01, -1.6174e-01, -1.0140e-01,  9.5657e-02,
          2.0798e-01, -4.9287e-02,  1.8534e-01, -3.8132e-02,  3.3299e-02,
          2.3485e-01, -2.2178e-01, -1.4854e-01, -5.7424e-02, -8.0627e-02,
          2.3284e-01],
        [-1.6088e-01, -1.2642e-01, -1.7955e-01, -2.3827e-01, -1.5002e-01,
          2.1309e-01,  1.0271e-01,  1.1824e-01,  4.3221e-03, -1.3177e-01,
          3.7039e-02, -2.3456e-01, -1.8739e-01, -1.2989e-01,  1.5085e-01,
          1.3757e-01],
        [-1.0339e-01, -2.0419e-02,  1.5736e-01,  2.4880e-01,  9.3352e-02,
          3.2795e-02,  1.5991e-01, -1.4515e-01,  3.8902e-02, -1.9798e-01,
         -1.8125e-01, -1.3133e-01,  1.0701e-01,  1.0171e-01, -1.5516e-01,
          6.5267e-02],
        [ 1.5294e-01, -2.9883e-02,  1.7127e-02,  6.7227e-02,  1.5244e-01,
          2.3664e-01, -1.9560e-01, -8.8826e-02,  9.5233e-02,  2.1759e-01,
          2.2949e-01,  2.3255e-01,  5.9521e-02, -2.0677e-01,  2.8521e-02,
         -1.5645e-01],
        [-2.1759e-01,  2.1914e-01,  1.8201e-01, -2.5681e-01,  5.1117e-02,
         -4.2812e-02, -3.9276e-02, -1.1080e-01,  9.7993e-02, -1.4894e-01,
          8.8584e-02,  1.2425e-01,  1.7668e-01,  9.3587e-02, -2.5786e-01,
         -1.7253e-01],
        [ 1.1889e-01,  4.3405e-02, -1.9846e-01, -1.4064e-01,  2.3663e-01,
          1.7118e-01, -9.9029e-02, -5.7003e-02, -2.2814e-01, -5.9339e-03,
         -1.9022e-01, -2.0209e-01, -1.5154e-02,  3.2674e-02, -1.0754e-01,
          1.5561e-01],
        [-1.4885e-01,  2.2133e-01,  1.7193e-01, -2.2944e-01, -5.2584e-02,
          1.0350e-02,  3.4796e-02,  5.6848e-02,  9.6400e-02,  1.0708e-02,
         -1.2644e-01,  1.1302e-01, -2.4234e-01, -1.4818e-01, -4.7940e-02,
         -1.2908e-01],
        [-9.3874e-02, -1.9739e-01, -4.7943e-02,  5.7088e-02, -1.6614e-01,
         -1.5875e-02,  1.7030e-01, -3.6420e-02, -1.7051e-03, -2.5839e-02,
          4.9228e-02,  1.4966e-01,  2.3250e-01,  1.4789e-01,  2.4547e-01,
         -7.8189e-04],
        [-2.2420e-01, -1.1860e-01,  1.6727e-01, -3.7974e-03, -1.2682e-01,
         -1.9142e-01, -2.3513e-01, -2.1135e-01, -5.1853e-02,  1.3744e-01,
          1.3559e-01, -2.4178e-01,  1.5616e-01, -1.9739e-01, -5.5996e-02,
         -1.0148e-01],
        [-3.3341e-02, -3.2794e-02, -2.2207e-01, -2.1606e-01, -4.4976e-02,
          4.3206e-03, -1.1306e-01,  8.9712e-02, -2.2456e-01, -1.5379e-02,
          2.2559e-01, -9.7292e-02,  2.2623e-01,  8.7381e-02, -2.3474e-01,
          1.5997e-01],
        [-3.3836e-02, -1.0127e-01,  2.0991e-01, -1.9179e-01,  3.3263e-02,
         -4.5728e-02,  1.7388e-01,  7.2055e-02,  1.1547e-01,  9.7147e-02,
         -5.0056e-02, -4.1491e-02, -1.9866e-01,  1.4438e-01,  2.0847e-01,
          1.8193e-01],
        [-1.7403e-01,  2.5624e-02, -1.7155e-01, -1.3383e-01, -1.3925e-01,
          8.9459e-02, -1.4006e-01,  2.3239e-04,  1.9385e-02,  1.7184e-01,
         -1.7662e-01, -1.6408e-01, -1.3386e-01,  1.8088e-01, -8.1287e-02,
          2.2398e-01],
        [ 9.8997e-02,  1.8008e-02, -1.0817e-02, -5.6263e-02,  2.0138e-02,
         -6.5735e-02,  8.1927e-03,  4.1148e-02, -1.8563e-01, -1.2886e-01,
          2.0191e-01, -2.0133e-01, -1.6258e-02,  2.5038e-01,  8.4480e-02,
         -6.0294e-03],
        [-2.2787e-01,  1.3608e-01, -1.6623e-01, -6.8743e-02, -8.5199e-02,
         -3.8130e-02, -5.1891e-03,  2.0051e-01,  2.3328e-02,  2.1116e-01,
          1.3864e-01, -1.6856e-01,  1.0095e-01, -1.8278e-01, -8.8565e-02,
          9.3488e-02],
        [ 8.7201e-02,  2.0342e-01, -7.8192e-02, -9.7219e-03,  1.1948e-01,
         -2.4267e-01,  1.7632e-01, -2.1071e-01, -9.5776e-04, -4.8345e-02,
         -1.3231e-01,  3.2720e-02,  2.0143e-01, -7.7577e-02, -1.6302e-01,
         -9.2182e-02],
        [-2.4017e-01,  1.0430e-01, -1.3650e-01, -1.8366e-01, -1.6096e-01,
          1.2586e-01,  1.3278e-01,  1.8892e-01,  9.1065e-02, -7.9640e-02,
         -6.3947e-02,  7.9761e-02,  2.0957e-01,  6.9434e-02, -1.2723e-01,
         -1.1047e-01],
        [-2.4454e-01,  5.2603e-02, -1.3482e-01, -2.1806e-01,  2.2254e-01,
         -1.5793e-01, -3.3097e-02,  6.7401e-02,  3.3155e-03, -1.7307e-01,
         -2.0691e-01,  1.9272e-01,  3.5453e-02,  2.0103e-01, -8.1247e-02,
          7.3574e-02],
        [-5.9546e-02, -2.9777e-04, -1.3844e-01,  9.5576e-02,  8.6778e-02,
         -6.0789e-04, -6.2878e-02, -1.6070e-01,  1.6623e-01, -1.8303e-01,
          1.0441e-01, -9.4931e-02, -1.2180e-01,  3.5146e-02, -1.1472e-01,
          7.2623e-02],
        [ 5.2124e-02, -1.7345e-01,  5.4545e-02,  1.0058e-01,  1.0526e-01,
         -3.6936e-02, -2.0369e-01, -4.1967e-02,  8.8015e-02, -9.3865e-02,
          8.9661e-02,  1.6263e-01, -1.3156e-01,  2.7352e-03,  1.2162e-01,
          8.6600e-03],
        [ 9.4698e-03,  3.5839e-02, -1.9789e-01,  1.8059e-02,  1.6945e-01,
          2.1948e-01,  1.3631e-01,  2.6281e-02,  1.0615e-01, -1.2196e-01,
         -2.0328e-01, -2.1145e-01,  2.4492e-01,  1.5578e-01, -1.7150e-01,
          1.0570e-01],
        [ 1.8421e-01,  2.5270e-01,  2.3101e-01,  2.0879e-01, -4.6305e-02,
         -7.4271e-02,  2.1266e-01,  1.4777e-01, -1.4307e-01,  2.2891e-01,
          1.2329e-01,  1.4279e-01, -1.5354e-01,  7.6429e-02, -7.8919e-02,
          1.9116e-01],
        [-4.2149e-02,  8.8812e-02,  3.4064e-02,  9.6485e-02, -9.7580e-02,
          1.0896e-01, -1.8864e-01,  4.3151e-02, -1.8313e-01, -2.5387e-01,
         -2.0415e-01,  1.9247e-01,  1.2788e-01,  2.2576e-01,  1.9010e-01,
         -2.2615e-01],
        [-1.7775e-01, -4.2131e-02, -1.5358e-01,  1.8578e-01, -1.7537e-01,
         -1.0569e-01, -2.3700e-01, -1.4215e-01,  2.1015e-01,  1.9921e-01,
          1.7211e-01,  1.8913e-01,  2.1946e-01, -6.6247e-02,  1.2121e-01,
          2.3719e-01],
        [ 8.4229e-02,  2.4664e-01,  1.4678e-01,  1.7022e-01, -7.0026e-02,
          1.3590e-01,  2.8578e-02, -5.5983e-02, -1.4115e-01, -1.3557e-01,
         -1.8956e-01,  1.6986e-01,  1.7844e-01, -2.4849e-02, -1.2691e-01,
          1.9674e-01],
        [ 1.7223e-01,  2.6345e-02, -1.1778e-01,  2.3956e-01,  1.0558e-01,
         -1.8032e-01,  2.4649e-01,  1.9622e-01, -8.3943e-02,  1.5193e-01,
         -1.2898e-01, -2.4810e-02,  2.2458e-01,  2.1182e-02, -1.9032e-01,
         -7.9325e-02],
        [-6.2945e-02, -5.8449e-02,  1.2532e-02,  2.2075e-01,  5.1885e-03,
         -1.5911e-01,  1.7987e-02,  1.2483e-01,  1.2917e-01, -2.2481e-01,
         -4.7599e-02, -1.8618e-01, -9.8850e-02,  9.3453e-02, -1.8749e-01,
          8.6012e-02],
        [ 2.2052e-01,  3.0461e-02, -1.8479e-01, -9.5754e-02,  3.4946e-02,
         -1.9337e-01,  1.2684e-01, -2.1125e-01, -2.3775e-01, -2.2563e-01,
         -3.1572e-02,  1.8658e-01, -2.2164e-01,  2.2158e-01, -1.2895e-01,
          6.1084e-02],
        [ 1.4914e-02, -2.2653e-01,  2.0328e-01,  1.2580e-01,  2.4016e-01,
          1.1736e-01, -1.7123e-01,  2.0400e-01,  7.2306e-03,  1.1133e-01,
         -2.0845e-01, -7.7777e-02, -2.5254e-01, -1.0069e-01,  4.6139e-02,
         -2.0884e-01],
        [ 6.6918e-02,  1.2076e-01,  1.0986e-02, -1.6996e-01, -2.0153e-01,
         -7.4052e-02,  1.1630e-01,  2.5566e-01,  1.5033e-01, -1.6999e-02,
          9.7010e-02, -2.3539e-01, -2.0306e-01,  1.2628e-01, -1.6866e-01,
          1.0296e-01],
        [-1.8699e-01, -1.1455e-01, -2.0500e-01,  1.2826e-01, -2.9339e-02,
          1.8750e-01,  1.4732e-01,  1.3571e-01,  6.4906e-04, -7.0374e-02,
         -4.2374e-02,  4.5121e-02,  1.3007e-01, -1.6805e-01,  2.0352e-01,
         -1.4589e-02],
        [-1.9949e-01, -1.5180e-01,  2.0691e-01,  2.1497e-01, -2.4268e-01,
          1.6746e-01,  1.2733e-01,  1.4887e-01,  2.1369e-01, -1.4650e-01,
          6.4244e-02,  8.9601e-02, -8.5666e-02,  7.4637e-02, -8.5341e-02,
          1.3757e-01]], requires_grad=True)

Operation: linear, Node: seq_blocks_4
  Weight Shape: [64, 32]
  Precision: [8, 4] bits
  Weight : Parameter containing:
tensor([[ 0.0224,  0.1474, -0.1240,  ..., -0.1188, -0.0096,  0.1450],
        [-0.0568, -0.1641,  0.0617,  ..., -0.0809,  0.0947, -0.0754],
        [-0.0868, -0.0162, -0.0158,  ..., -0.1618, -0.1322,  0.0759],
        ...,
        [ 0.1006,  0.1655, -0.1205,  ..., -0.1700,  0.1280, -0.1308],
        [-0.0044,  0.0273,  0.0577,  ..., -0.0340,  0.0031, -0.0549],
        [-0.1120, -0.1265,  0.1155,  ...,  0.1334,  0.0941,  0.0208]],
       requires_grad=True)

Operation: linear, Node: seq_blocks_6
  Weight Shape: [32, 64]
  Precision: [8, 4] bits
  Weight : Parameter containing:
tensor([[-0.0243, -0.0506,  0.0695,  ..., -0.1145, -0.0460, -0.0724],
        [-0.0733,  0.1067, -0.0335,  ...,  0.1061, -0.0300,  0.0499],
        [-0.0338, -0.0401, -0.0624,  ..., -0.0611, -0.1121,  0.0790],
        ...,
        [-0.0274, -0.0959,  0.1080,  ..., -0.0567,  0.0701,  0.0236],
        [-0.0093, -0.1025,  0.1283,  ...,  0.0770, -0.0065,  0.0090],
        [ 0.1003,  0.0592,  0.0989,  ..., -0.1072,  0.0992,  0.0348]],
       requires_grad=True)

Operation: linear, Node: seq_blocks_9
  Weight Shape: [16, 32]
  Precision: [8, 4] bits
  Weight : Parameter containing:
tensor([[-0.0151,  0.0629, -0.0714, -0.0691, -0.0203, -0.0949,  0.1023,  0.0968,
         -0.1384,  0.0510, -0.0721, -0.0949, -0.0323, -0.0455, -0.1065,  0.0245,
          0.0987, -0.0396, -0.0278, -0.0007, -0.0840,  0.0887,  0.1648, -0.0130,
          0.0069,  0.1275,  0.1390, -0.1419, -0.0927, -0.1654,  0.1248,  0.1437],
        [-0.0658,  0.0789,  0.0326, -0.1397,  0.0642,  0.0934,  0.0238,  0.0045,
          0.0313,  0.1326,  0.1501,  0.0227,  0.0810, -0.0324,  0.0855, -0.0628,
         -0.1457,  0.1041,  0.0805,  0.0192,  0.0710,  0.0154,  0.0509,  0.0735,
         -0.0893, -0.0252,  0.0180, -0.0065, -0.1466,  0.0299,  0.0932, -0.0981],
        [ 0.1565,  0.0520,  0.0978, -0.0765, -0.1412, -0.0772, -0.0025, -0.0899,
          0.1480, -0.0896, -0.1143, -0.0425,  0.1770, -0.0742,  0.1528,  0.0750,
         -0.0059,  0.0884,  0.0907, -0.1024, -0.0209,  0.1724,  0.1228, -0.0923,
         -0.0324,  0.0889,  0.0760,  0.0059, -0.1011, -0.1354, -0.0193,  0.0542],
        [-0.0200, -0.1310, -0.0266,  0.1597, -0.1529, -0.1263,  0.1196,  0.1858,
         -0.1798, -0.1422, -0.0731,  0.1544, -0.0881,  0.0095,  0.1291, -0.0803,
         -0.0366, -0.1018,  0.0222,  0.1851,  0.0545,  0.1140, -0.0045, -0.0469,
         -0.0414,  0.1163, -0.0095, -0.1522, -0.0523,  0.0696,  0.1794, -0.0509],
        [-0.1382, -0.1315, -0.1442,  0.0875,  0.1401,  0.0456,  0.0647,  0.1006,
          0.1238,  0.0991,  0.1107,  0.0414,  0.0973, -0.1151, -0.1111,  0.0600,
          0.0185, -0.1485, -0.1188, -0.1582,  0.1564,  0.1057, -0.0999,  0.0382,
         -0.0231,  0.1406, -0.1751, -0.1075, -0.1470,  0.0835, -0.0936,  0.1097],
        [ 0.0626, -0.1014, -0.0364, -0.0902,  0.0924, -0.0889,  0.0429, -0.0091,
         -0.1215, -0.0918, -0.1028,  0.1008,  0.0439,  0.0003, -0.0923, -0.1558,
         -0.0652, -0.1485, -0.1750, -0.1855, -0.0972, -0.1646,  0.0432,  0.0901,
          0.0762, -0.1974,  0.0887,  0.0167,  0.0751, -0.0515,  0.0972, -0.1536],
        [ 0.0655,  0.1255,  0.0661, -0.1723, -0.0475, -0.0333,  0.0819, -0.0542,
          0.0266, -0.0720, -0.0238, -0.0048, -0.1071, -0.1280,  0.1577,  0.1392,
         -0.0157,  0.1461,  0.1293, -0.0956, -0.0626, -0.1385,  0.0498,  0.0158,
         -0.0263,  0.1478, -0.0562, -0.0991, -0.1209, -0.0933,  0.1902, -0.0758],
        [-0.0368,  0.0696, -0.1362, -0.1170,  0.1465, -0.0721, -0.0935, -0.0378,
         -0.0129,  0.0358,  0.0771, -0.1423,  0.1643, -0.1603,  0.0631, -0.0421,
         -0.0890,  0.1581, -0.1576,  0.0051,  0.1282,  0.1414, -0.1480,  0.0417,
          0.1689, -0.1849,  0.0312, -0.0990,  0.0222, -0.1647,  0.1000, -0.0320],
        [ 0.1146, -0.0091, -0.0351, -0.0476, -0.1503, -0.0691,  0.0015, -0.1197,
          0.1609, -0.1300, -0.0591, -0.0349, -0.1566, -0.0044, -0.0723, -0.1351,
         -0.0097,  0.1425,  0.0303,  0.1626,  0.0047, -0.0638,  0.1236, -0.0868,
         -0.1757,  0.0484,  0.1049, -0.1106,  0.0500, -0.0494, -0.0078, -0.0305],
        [-0.0202, -0.1629,  0.0483, -0.0642,  0.1468,  0.0174, -0.1626, -0.1075,
         -0.0812,  0.0886, -0.1110, -0.0738,  0.0252, -0.1380, -0.0633,  0.1717,
         -0.0942, -0.0729,  0.1161,  0.0638,  0.1269, -0.1372,  0.0897, -0.0232,
         -0.1253, -0.0487, -0.1488,  0.1014, -0.0254, -0.1719,  0.1710,  0.1225],
        [ 0.0839,  0.1723,  0.1386,  0.1747,  0.1628,  0.1451,  0.0459, -0.0204,
          0.0132, -0.0960,  0.0984, -0.0021,  0.0271, -0.1074, -0.0678,  0.1560,
          0.0073,  0.0557, -0.1602, -0.0814,  0.0037, -0.0177,  0.0895, -0.0300,
          0.1443, -0.0425,  0.0662, -0.1224,  0.0189, -0.1665, -0.1568, -0.0945],
        [ 0.0568,  0.0228,  0.0098, -0.0138,  0.1574,  0.1922,  0.0849,  0.1300,
          0.0007, -0.0751,  0.0438,  0.1041, -0.0373, -0.1119,  0.0512, -0.0393,
         -0.0786,  0.1194, -0.0060, -0.1116, -0.0905,  0.1706,  0.1012, -0.1143,
          0.0028, -0.0156,  0.0844, -0.1295, -0.0092,  0.0565,  0.0756,  0.0222],
        [-0.0757, -0.1542,  0.1146,  0.1393, -0.0704, -0.1658,  0.0355, -0.1351,
         -0.1003,  0.0667, -0.0120, -0.0011,  0.1601, -0.0113,  0.1014, -0.0453,
          0.1321,  0.0360,  0.1462,  0.0508, -0.0727,  0.0812,  0.1630,  0.0530,
          0.0735, -0.1327,  0.0816, -0.1886,  0.1394,  0.0904,  0.0306, -0.1466],
        [ 0.1746,  0.1248,  0.1650,  0.0889,  0.0530, -0.0394, -0.0519,  0.0675,
          0.0529,  0.0109,  0.0115,  0.0594, -0.1551, -0.1586, -0.0688,  0.1362,
          0.0927, -0.1487, -0.0316, -0.0225,  0.0452,  0.1456,  0.0085, -0.1386,
          0.0140, -0.0702, -0.0972,  0.1189, -0.0094,  0.1007,  0.1170,  0.0286],
        [ 0.1043, -0.0646,  0.1036,  0.0130,  0.1028,  0.1769, -0.1129,  0.1571,
          0.1234, -0.1320,  0.0350, -0.1474,  0.0612,  0.0264, -0.1839,  0.0619,
         -0.0401,  0.1868,  0.1836,  0.0498,  0.1229,  0.0531,  0.1748, -0.0223,
          0.0508, -0.0382, -0.0036, -0.0327, -0.0596,  0.1098,  0.0628, -0.0763],
        [-0.1399,  0.1045, -0.1454, -0.0053, -0.1587,  0.1436,  0.1536, -0.1591,
         -0.1658, -0.0865,  0.0476, -0.0798, -0.0315, -0.0396,  0.0192, -0.0796,
          0.0950, -0.1186, -0.0061,  0.1325, -0.0947,  0.1295,  0.1021, -0.0390,
          0.0742, -0.1366,  0.0024,  0.0607,  0.1604, -0.0910, -0.1119, -0.0864]],
       requires_grad=True)

Operation: linear, Node: seq_blocks_12
  Weight Shape: [5, 16]
  Precision: [8, 4] bits
  Weight : Parameter containing:
tensor([[-0.0514,  0.0585,  0.0570, -0.0256, -0.1943,  0.2167,  0.1327, -0.2331,
         -0.1954,  0.2544,  0.0489, -0.0591,  0.1941,  0.0809,  0.1007,  0.2129],
        [-0.0302,  0.2325, -0.1862, -0.1640,  0.0371, -0.0474,  0.1380, -0.0931,
          0.0352, -0.0756,  0.1955, -0.0105, -0.0100,  0.1886,  0.2487,  0.2606],
        [-0.2664, -0.1637, -0.0359, -0.0510,  0.1750,  0.1313, -0.0324,  0.0333,
          0.0387,  0.2344,  0.1532, -0.1167, -0.1291,  0.1081, -0.0146, -0.1572],
        [-0.2620,  0.0460, -0.0914, -0.1494,  0.0692,  0.2619,  0.2250,  0.1708,
         -0.2033,  0.1608,  0.1204,  0.0763, -0.0810, -0.0036, -0.1394, -0.0677],
        [-0.0067, -0.0162, -0.2327,  0.1228, -0.1875, -0.1668,  0.1342, -0.1909,
          0.2721,  0.2633, -0.0714, -0.0170, -0.1781,  0.0303,  0.0653, -0.1263]],
       requires_grad=True)
```

#### 7.
```
-----+
INFO     Initialising model 'jsc-lch'...
INFO     Initialising dataset 'jsc'...
INFO     Project will be created at /home/lch121600/ADLSlab/mase/mase_output/jsc-lch
INFO     Transforming model 'jsc-lch'...
INFO     Loaded pytorch lightning checkpoint from /home/lch121600/ADLSlab/mase/mase_output/jsc-lch_classification_jsc_2024-02-06/software/training_ckpts/best.ckpt
INFO     Quantized graph histogram:
INFO
| Original type   | OP           |   Total |   Changed |   Unchanged |
|-----------------+--------------+---------+-----------+-------------|
| BatchNorm1d     | batch_norm1d |       3 |         0 |           3 |
| Linear          | linear       |       5 |         5 |           0 |
| ReLU            | relu         |       6 |         0 |           6 |
| output          | output       |       1 |         0 |           1 |
| x               | placeholder  |       1 |         0 |           1 |
INFO     Saved mase graph to /home/lch121600/ADLSlab/mase/mase_output/jsc-lch/software/transform/transformed_ckpt
INFO     Transformation is completed
```

#### 8. Optional
```python
from chop.passes.graph.analysis.flop_estimator.calculator.calc_modules import calculate_modules
from chop.passes.graph.analysis.quantization.calculate_avg_bits import calculate_avg_bits_mg_analysis_pass
from chop.ir.graph.mase_graph import MaseGraph


def FLOP_count(graph, pass_args: dict):
    flop = 0
    for node in graph.fx_graph.nodes:
        mase_meta = node.meta["mase"].parameters
        mase_op = mase_meta["common"]["mase_op"]
        mase_type = mase_meta["common"]["mase_type"]
        if mase_type in ["module", "module_related_func"]:
             m = mase_meta["common"]["args"]["data_in_0"]["type"]
             if m in ["float"]:
            #    n_modual = mase_meta["common"]["mase_type"]["module"]
               in_data = mase_meta["common"]["args"]["data_in_0"]["value"]
               out_data = mase_meta["common"]["results"]["data_out_0"]["value"]
            
               result = calculate_modules(node.meta["mase"].module, [in_data], [out_data])
               if (result != None):
                 flop += (result["computations"] +result["backward_computations"])

               print(flop)

    return flop
f = FLOP_count(mg, pass_args)
print (f)
```
This function used built-in function `calculate_modules` to compute the flop operation. For my own model the quatized model's flop is 252600.
The original one is 4932600.


```

```
